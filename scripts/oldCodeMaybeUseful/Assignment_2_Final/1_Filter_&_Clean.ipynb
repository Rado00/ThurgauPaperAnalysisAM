{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color =\"gold\"> Filter & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matsim\n",
    "import os\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from my_matsim_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"gold\"> Choose Population % to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THIS FOR THE PATH\n",
    "pct = 100\n",
    "place = \"Frauenfeld\"\n",
    "# place = \"Weinfelden\"\n",
    "# place = \"Thurgau\"\n",
    "scenario_name = \"BaselineScenario\"\n",
    "simulation_name = \"Frauenfeld_Baseline_100pct_11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "def get_data_folder_path():\n",
    "    # Get the current operating system\n",
    "    os_type = platform.system()\n",
    "    user_name = os.getlogin()\n",
    "\n",
    "    # Define data folder paths for different systems\n",
    "    if os_type == 'Windows' and user_name == 'muaa':\n",
    "        data_folder_path = f\"C://Users//{user_name}//Documents//3_MIEI//2023_ABMT_Data//{place}//\"    \n",
    "    elif os_type == 'Linux' and user_name == 'comura':\n",
    "        data_folder_path = '/cluster/home/comura/.../'\n",
    "    elif os_type == 'Linux' and user_name == 'cmuratori':\n",
    "        data_folder_path = '/cluster/home/cmuratori/.../'\n",
    "    elif os_type == 'Linux' and user_name == 'muaa':\n",
    "        data_folder_path = '/cluster/home/muaa/.../'\n",
    "    else:\n",
    "        raise Exception(\"Unsupported system configuration\")\n",
    "\n",
    "    return data_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = get_data_folder_path()\n",
    "scenario_path = f\"{data_folder_path}//{scenario_name}//{pct}pct//\"\n",
    "output_folder_path = f\"{data_folder_path}//{simulation_name}//\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"gold\"> READ ALL THE \"0_Analysis\" CSVs\n",
    "Takes around 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muaa\\AppData\\Local\\Temp\\ipykernel_20220\\3201299563.py:6: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_activity_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_activity_synt.csv')\n",
      "C:\\Users\\muaa\\AppData\\Local\\Temp\\ipykernel_20220\\3201299563.py:9: DtypeWarning: Columns (0,4,8,13,14,15,16,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_persons_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_persons_synt.csv')\n",
      "C:\\Users\\muaa\\AppData\\Local\\Temp\\ipykernel_20220\\3201299563.py:10: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_routes_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_routes_synt.csv')\n",
      "C:\\Users\\muaa\\AppData\\Local\\Temp\\ipykernel_20220\\3201299563.py:15: DtypeWarning: Columns (0,4,9,14,15,16,17,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_persons_sim = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_persons_sim.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load dataframes that are in common between SynPop and Simulation Outputs\n",
    "df_households_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_households_synt.csv')\n",
    "network_geo = gpd.read_file(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/network_geo.geojson')\n",
    "\n",
    "# Load _synth dataframes\n",
    "df_activity_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_activity_synt.csv')\n",
    "# df_plans_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_plans_synt.csv')\n",
    "df_legs_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_legs_synt.csv')\n",
    "df_persons_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_persons_synt.csv')\n",
    "df_routes_synt = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_routes_synt.csv')\n",
    "\n",
    "# Load _sim dataframes\n",
    "df_activity_sim = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_activity_sim.csv')\n",
    "df_legs_sim = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_legs_sim.csv')\n",
    "df_persons_sim = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_persons_sim.csv')\n",
    "df_routes_sim = pd.read_csv(f'{data_folder_path}/0_PreProcess_CSVs_SyntheticAndOutputs/{pct}pct/df_routes_sim.csv')\n",
    "\n",
    "#Load Microcensus dataframes\n",
    "df_population_mic = pd.read_csv(os.path.join(data_folder_path,'0_Microcensus//population.csv'))\n",
    "df_trips_mic = pd.read_csv(os.path.join(data_folder_path,'0_Microcensus//trips.csv'))\n",
    "df_personen_geschlecht = pd.read_csv(os.path.join(data_folder_path,'0_Microcensus//Personen_ZH_Sex.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The network contains information about the coordinate system it is expressed in, which is useful eg. to add map backgrounds\n",
    "network_geo.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Microzensus - Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Real Population ZH with unknown sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'SexLang' is 'unbekannt'\n",
    "df_personen_geschlecht = df_personen_geschlecht[df_personen_geschlecht['SexLang'] != 'unbekannt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Cleaning of Synthetic Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_data(df):\n",
    "    \"\"\"\n",
    "    Function to convert 'dep_time' and 'trav_time' from string to timedelta,\n",
    "    then to seconds, and calculate 'arrival_time'.\n",
    "    \"\"\"\n",
    "    # Convert 'dep_time' from string to timedelta\n",
    "    df['dep_time'] = pd.to_timedelta(df['dep_time'])\n",
    "\n",
    "    # Convert 'dep_time' from timedelta to seconds\n",
    "    df['departure_time'] = df['dep_time'].dt.total_seconds().astype(int)\n",
    "\n",
    "    # Convert 'trav_time' from string to timedelta\n",
    "    df['trav_time'] = pd.to_timedelta(df['trav_time'])\n",
    "\n",
    "    # Convert 'trav_time' from timedelta to seconds\n",
    "    df['trav_time_seconds'] = df['trav_time'].dt.total_seconds().astype(int)\n",
    "\n",
    "    # Calculate 'arrival_time_seconds' by adding 'trav_time_seconds' to 'dep_time_seconds'\n",
    "    df['arrival_time'] = df['departure_time'] + df['trav_time_seconds']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run the function for both df_legs_synt and df_legs_sim\n",
    "df_legs_synt = process_time_data(df_legs_synt)\n",
    "df_legs_sim = process_time_data(df_legs_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column 'id' to 'person_id'\n",
    "df_persons_synt.rename(columns={'id': 'hh_id'}, inplace=True)\n",
    "df_persons_sim.rename(columns={'id': 'hh_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AROUND 21 minutes\n",
    "\n",
    "def map_person_id_to_activities(df_activities, df_persons, activity_type='Home'):\n",
    "    \"\"\"\n",
    "    Map household IDs from df_persons to df_activities based on home coordinates and \n",
    "    propagate the ID to other activities in the same plan.\n",
    "\n",
    "    :param df_activities: DataFrame containing activities with coordinates (x, y) and plan_id.\n",
    "    :param df_persons: DataFrame containing person data with home coordinates (home_x, home_y) and hh_id.\n",
    "    :param activity_type: The type of activity used to map person IDs (default 'Home').\n",
    "    :return: DataFrame with person_id mapped and propagated.\n",
    "    \"\"\"\n",
    "    # Ensure coordinates are of type float64\n",
    "    df_activities['x'] = pd.to_numeric(df_activities['x'], errors='coerce')\n",
    "    df_activities['y'] = pd.to_numeric(df_activities['y'], errors='coerce')\n",
    "    df_persons['home_x'] = pd.to_numeric(df_persons['home_x'], errors='coerce')\n",
    "    df_persons['home_y'] = pd.to_numeric(df_persons['home_y'], errors='coerce')\n",
    "\n",
    "    # Filter df_activities for rows where type is the specified activity type\n",
    "    home_activities = df_activities[df_activities['type'] == activity_type]\n",
    "\n",
    "    # Merge the household IDs from df_persons to home_activities based on coordinate match\n",
    "    merged_home_activities = pd.merge(\n",
    "        home_activities,\n",
    "        df_persons[['hh_id', 'home_x', 'home_y']],\n",
    "        left_on=['x', 'y'],\n",
    "        right_on=['home_x', 'home_y'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Create a mapping of plan_id to hh_id\n",
    "    if merged_home_activities['plan_id'].is_unique:\n",
    "        plan_id_to_hh_id = merged_home_activities.set_index('plan_id')['hh_id']\n",
    "    else:\n",
    "        plan_id_to_hh_id = merged_home_activities.groupby('plan_id')['hh_id'].first()\n",
    "\n",
    "    # Map the hh_id as person_id to all activities in df_activities\n",
    "    df_activities['person_id'] = df_activities['plan_id'].map(plan_id_to_hh_id)\n",
    "\n",
    "    # Propagate the person_id to other activities in the same plan\n",
    "    df_activities['person_id'] = df_activities.groupby('plan_id')['person_id'].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    return df_activities\n",
    "\n",
    "# Example usage:\n",
    "df_activity_synt_filtered = map_person_id_to_activities(df_activity_synt, df_persons_synt)\n",
    "df_activity_sim_filtered = map_person_id_to_activities(df_activity_sim, df_persons_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_activity_and_legs_data(df_activity, df_legs, values_to_remove, modes_to_remove):\n",
    "    # Identify persons with only one 'Home' activity initially\n",
    "    initial_single_home = df_activity.groupby('person_id').filter(lambda x: len(x) == 1 and x['type'].eq('Home').all())\n",
    "\n",
    "    # Filter the activity DataFrame\n",
    "    df_activity_filtered = df_activity[~df_activity['type'].isin(values_to_remove)]\n",
    "\n",
    "    # Find all 'plan_id' values where 'type' is 'outside'\n",
    "    plan_ids_to_remove = df_activity_filtered[df_activity_filtered['type'] == 'outside']['plan_id'].unique()\n",
    "\n",
    "    # Filter out all rows with these 'plan_id' values\n",
    "    df_activity_filtered = df_activity_filtered[~df_activity_filtered['plan_id'].isin(plan_ids_to_remove)]\n",
    "\n",
    "    # Additional filter to remove 'outside'\n",
    "    df_activity_filtered = df_activity_filtered[~df_activity_filtered['type'].isin(['outside'])]\n",
    "\n",
    "    # Combine 'Access Walk' and 'Egress Walk' into 'Walk' in legs DataFrame\n",
    "    df_legs['mode'] = df_legs['mode'].replace({'access_walk': 'walk', 'egress_walk': 'walk'})\n",
    "\n",
    "    # Remove specified modes from the legs DataFrame\n",
    "    df_legs_filtered = df_legs[~df_legs['mode'].isin(modes_to_remove)]\n",
    "\n",
    "    # Identify persons who now only have one 'Home' activity\n",
    "    final_single_home = df_activity_filtered.groupby('person_id').filter(lambda x: len(x) == 1 and x['type'].eq('Home').all())\n",
    "\n",
    "    # Exclude persons who initially had only one 'Home' activity\n",
    "    final_single_home = final_single_home[~final_single_home['person_id'].isin(initial_single_home['person_id'])]\n",
    "\n",
    "    # Remove these persons from the filtered data\n",
    "    df_activity_filtered = df_activity_filtered[~df_activity_filtered['person_id'].isin(final_single_home['person_id'])]\n",
    "\n",
    "    return df_activity_filtered, df_legs_filtered\n",
    "\n",
    "# Define the values to be removed for synthetic and simulated data\n",
    "values_to_remove = ['freight_unloading', 'freight_loading', 'pt interaction']\n",
    "modes_to_remove = ['truck', 'outside']\n",
    "\n",
    "# Process synthetic data\n",
    "df_activity_synt_filtered, df_legs_synt_filtered = process_activity_and_legs_data(df_activity_synt, df_legs_synt, values_to_remove, modes_to_remove)\n",
    "\n",
    "# Process simulated data\n",
    "df_activity_sim_filtered, df_legs_sim_filtered = process_activity_and_legs_data(df_activity_sim, df_legs_sim, values_to_remove, modes_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity_synt = df_activity_synt_filtered\n",
    "df_activity_sim = df_activity_sim_filtered\n",
    "\n",
    "df_legs_synt = df_legs_synt_filtered\n",
    "df_legs_sim = df_legs_sim_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes New Trips Based on Activity End And Start Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trips_dataframe(df_activity):\n",
    "    # List to hold new trip entries\n",
    "    new_trips = []\n",
    "\n",
    "    # Iterate over the activity DataFrame\n",
    "    for i in range(len(df_activity) - 1):\n",
    "        # Get current and next row\n",
    "        current_row = df_activity.iloc[i]\n",
    "        next_row = df_activity.iloc[i + 1]\n",
    "\n",
    "        # Check if the IDs are consecutive\n",
    "        if current_row['id'] + 1 == next_row['id']:\n",
    "            # Create a new trip entry\n",
    "            new_trips.append({\n",
    "                'trip_id': current_row['id'],\n",
    "                'departure_time': current_row['end_time'],\n",
    "                'arrival_time': next_row['start_time'],\n",
    "                'start_coor_x': current_row['x'],\n",
    "                'start_coor_y': current_row['y'],\n",
    "                'ziel_coor_x': next_row['x'],\n",
    "                'ziel_coor_y': next_row['y'],\n",
    "                \n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the list of new trips\n",
    "    df_trips = pd.DataFrame(new_trips)\n",
    "\n",
    "    return df_trips\n",
    "\n",
    "\n",
    "df_trips_synt = create_trips_dataframe(df_activity_synt)\n",
    "df_trips_sim = create_trips_dataframe(df_activity_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_synt = df_trips_synt.dropna()\n",
    "df_trips_sim = df_trips_sim.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert_time(time_str):\n",
    "    try:\n",
    "        # Convert to datetime, then to time, and floor to 30-minute bins\n",
    "        return pd.to_datetime(time_str, format='%H:%M:%S', errors='raise').floor('30T').time()\n",
    "    except ValueError:\n",
    "        # Handle invalid time data (e.g., return None or a default time)\n",
    "        return None\n",
    "\n",
    "# Apply the conversion function to the DataFrame\n",
    "df_trips_synt['departure_time'] = df_trips_synt['departure_time'].apply(safe_convert_time)\n",
    "df_trips_synt['arrival_time'] = df_trips_synt['arrival_time'].apply(safe_convert_time)\n",
    "\n",
    "df_trips_sim['departure_time'] = df_trips_sim['departure_time'].apply(safe_convert_time)\n",
    "df_trips_sim['arrival_time'] = df_trips_sim['arrival_time'].apply(safe_convert_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert seconds to datetime and resample times to 15-minute bins\n",
    "df_trips_mic['departure_time'] = pd.to_datetime(df_trips_mic['departure_time'], unit='s').dt.floor('30T').dt.time\n",
    "df_trips_mic['arrival_time'] = pd.to_datetime(df_trips_mic['arrival_time'], unit='s').dt.floor('30T').dt.time\n",
    "\n",
    "# # Convert seconds to datetime and resample times to 30-minute bins for df_legs_synt\n",
    "# df_legs_synt['departure_time'] = pd.to_datetime(df_legs_synt['departure_time'], unit='s').dt.floor('30T').dt.time\n",
    "# df_legs_synt['arrival_time'] = pd.to_datetime(df_legs_synt['arrival_time'], unit='s').dt.floor('30T').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize and remove underscores from mode names for both dataframes\n",
    "df_trips_mic['mode'] = df_trips_mic['mode'].str.replace('_', ' ').str.title()\n",
    "df_legs_synt['mode'] = df_legs_synt['mode'].str.replace('_', ' ').str.title()\n",
    "df_legs_sim['mode'] = df_legs_sim['mode'].str.replace('_', ' ').str.title()\n",
    "\n",
    "# Capitalize and remove underscores from type names\n",
    "df_trips_mic['purpose'] = df_trips_mic['purpose'].str.replace('_', ' ').str.title()\n",
    "df_activity_synt['type'] = df_activity_synt['type'].str.replace('_', ' ').str.title()\n",
    "df_activity_sim['type'] = df_activity_sim['type'].str.replace('_', ' ').str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'age' is NaN\n",
    "df_persons_synt = df_persons_synt.dropna(subset=['age'])\n",
    "df_persons_synt['age'] = df_persons_synt['age'].astype(int)\n",
    "\n",
    "df_persons_sim = df_persons_sim.dropna(subset=['age'])\n",
    "df_persons_sim['age'] = df_persons_sim['age'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out persons younger than 6\n",
    "df_persons_synt = df_persons_synt[df_persons_synt['age'] >= 6]\n",
    "df_persons_sim = df_persons_sim[df_persons_sim['age'] >= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all numbers count car larger equal 3 to 3+\n",
    "def group_cars(value):\n",
    "    # Convert to integer if the value is a string\n",
    "    try:\n",
    "        value_int = int(value)\n",
    "    except ValueError:\n",
    "        # Return the value as is if it's not a number\n",
    "        return value\n",
    "\n",
    "    # Group all values less than or equal to 3 into '3+'\n",
    "    if value_int >= 3:\n",
    "        return '3+'\n",
    "    else:\n",
    "        return str(value_int)\n",
    "\n",
    "# Apply the grouping function to the 'number_of_cars' column\n",
    "df_population_mic['number_of_cars'] = df_population_mic['number_of_cars'].apply(group_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping '0' to 'male' and '1' to 'female'\n",
    "df_population_mic['sex'] = df_population_mic['sex'].replace({0: 'male', 1: 'female'})\n",
    "df_persons_synt['sex'] = df_persons_synt['sex'].replace({'m': 'male', 'f': 'female'})\n",
    "df_persons_sim['sex'] = df_persons_sim['sex'].replace({'m': 'male', 'f': 'female'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Activity Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create activity chains\n",
    "def create_activity_chain_mic(group):\n",
    "    chain = '-'.join(['H'] + [purpose[0] for purpose in group['purpose'].tolist()])  # Add 'H' at the start of each chain\n",
    "    return pd.Series({'activity_chain': chain})\n",
    "\n",
    "# Function to create uppercase activity chains\n",
    "def create_activity_chain_syn(group):\n",
    "    chain = '-'.join([purpose[0].upper() for purpose in group['type'].tolist()])\n",
    "    return pd.Series({'activity_chain': chain})\n",
    "\n",
    "# Create activity chains\n",
    "\n",
    "\n",
    "df_activity_chains_mic =  df_trips_mic.groupby(['person_id']).apply(create_activity_chain_mic).reset_index()\n",
    "df_activity_chains_syn =  df_activity_synt_filtered.groupby(['plan_id']).apply(create_activity_chain_syn).reset_index()\n",
    "df_activity_chains_sim =  df_activity_sim_filtered.groupby(['plan_id']).apply(create_activity_chain_syn).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all 'H' with 'H-H' in the 'activity_chain' column\n",
    "df_activity_chains_syn['activity_chain'] = df_activity_chains_syn['activity_chain'].replace({'H': 'H-H'})\n",
    "df_activity_chains_sim['activity_chain'] = df_activity_chains_sim['activity_chain'].replace({'H': 'H-H'})\n",
    "df_activity_chains_mic['activity_chain'] = df_activity_chains_mic['activity_chain'].replace({'H': 'H-H'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'household_weight' column from df_population_mic to df_trips_mic based on 'person_id'\n",
    "df_trips_mic = pd.merge(df_trips_mic, df_population_mic[['person_id', 'household_weight']], on='person_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'HHNR', 'WEGNR', 'f51100', 'f51400', 'wzweck1', 'wzweck2',\n",
       "       'wmittel', 'S_X_CH1903', 'S_Y_CH1903', 'Z_X_CH1903', 'Z_Y_CH1903',\n",
       "       'W_X_CH1903', 'W_Y_CH1903', 'w_rdist', 'dauer2', 'mode',\n",
       "       'mode_detailed', 'purpose', 'departure_time', 'arrival_time',\n",
       "       'person_id', 'trip_id', 'destination_x', 'destination_y', 'origin_x',\n",
       "       'origin_y', 'home_x', 'home_y', 'crowfly_distance', 'previous_trip_id',\n",
       "       'activity_duration', 'parking_cost', 'network_distance', 'origin_point',\n",
       "       'destination_point', 'household_weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips_mic.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_mic.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/trips_mic.csv', index=False)\n",
    "df_trips_synt.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/trips_synt.csv', index=False)\n",
    "df_trips_sim.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/trips_sim.csv', index=False)\n",
    "\n",
    "df_activity_chains_syn.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/activity_chains_syn.csv', index=False)\n",
    "df_activity_chains_sim.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/activity_chains_sim.csv', index=False)\n",
    "df_activity_chains_mic.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/activity_chains_mic.csv', index=False)\n",
    "\n",
    "df_population_mic.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/population_clean_mic.csv', index=False)\n",
    "df_persons_synt.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/population_clean_synth.csv', index=False)\n",
    "df_persons_sim.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/population_clean_sim.csv', index=False)\n",
    "\n",
    "df_legs_synt.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/legs_clean_synt.csv', index=False)\n",
    "df_legs_sim.to_csv(f'{data_folder_path}/1_Cleaned_CSVs/{pct}pct/legs_clean_sim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e844146b4402922edc282834f163f9a0d641b235707d4d6937c70f86c82d4552"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
